{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3731fa9-d118-4fd3-bf88-eab5e8857750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted and saved article blackassign0001\n",
      "Successfully extracted and saved article blackassign0002\n",
      "Successfully extracted and saved article blackassign0003\n",
      "Successfully extracted and saved article blackassign0004\n",
      "Successfully extracted and saved article blackassign0005\n",
      "Successfully extracted and saved article blackassign0006\n",
      "Successfully extracted and saved article blackassign0007\n",
      "Successfully extracted and saved article blackassign0008\n",
      "Successfully extracted and saved article blackassign0009\n",
      "Successfully extracted and saved article blackassign0010\n",
      "Successfully extracted and saved article blackassign0011\n",
      "Successfully extracted and saved article blackassign0012\n",
      "Successfully extracted and saved article blackassign0013\n",
      "Successfully extracted and saved article blackassign0014\n",
      "Successfully extracted and saved article blackassign0015\n",
      "Successfully extracted and saved article blackassign0016\n",
      "Successfully extracted and saved article blackassign0017\n",
      "Successfully extracted and saved article blackassign0018\n",
      "Successfully extracted and saved article blackassign0019\n",
      "Successfully extracted and saved article blackassign0020\n",
      "Successfully extracted and saved article blackassign0021\n",
      "Successfully extracted and saved article blackassign0022\n",
      "Successfully extracted and saved article blackassign0023\n",
      "Successfully extracted and saved article blackassign0024\n",
      "Successfully extracted and saved article blackassign0025\n",
      "Successfully extracted and saved article blackassign0026\n",
      "Successfully extracted and saved article blackassign0027\n",
      "Successfully extracted and saved article blackassign0028\n",
      "Successfully extracted and saved article blackassign0029\n",
      "Successfully extracted and saved article blackassign0030\n",
      "Successfully extracted and saved article blackassign0031\n",
      "Successfully extracted and saved article blackassign0032\n",
      "Successfully extracted and saved article blackassign0033\n",
      "Successfully extracted and saved article blackassign0034\n",
      "Successfully extracted and saved article blackassign0035\n",
      "Failed to extract article blackassign0036 from https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/: 'NoneType' object has no attribute 'get_text'\n",
      "Successfully extracted and saved article blackassign0037\n",
      "Successfully extracted and saved article blackassign0038\n",
      "Successfully extracted and saved article blackassign0039\n",
      "Successfully extracted and saved article blackassign0040\n",
      "Successfully extracted and saved article blackassign0041\n",
      "Successfully extracted and saved article blackassign0042\n",
      "Successfully extracted and saved article blackassign0043\n",
      "Successfully extracted and saved article blackassign0044\n",
      "Successfully extracted and saved article blackassign0045\n",
      "Successfully extracted and saved article blackassign0046\n",
      "Successfully extracted and saved article blackassign0047\n",
      "Successfully extracted and saved article blackassign0048\n",
      "Failed to extract article blackassign0049 from https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/: 'NoneType' object has no attribute 'get_text'\n",
      "Successfully extracted and saved article blackassign0050\n",
      "Successfully extracted and saved article blackassign0051\n",
      "Successfully extracted and saved article blackassign0052\n",
      "Successfully extracted and saved article blackassign0053\n",
      "Successfully extracted and saved article blackassign0054\n",
      "Successfully extracted and saved article blackassign0055\n",
      "Successfully extracted and saved article blackassign0056\n",
      "Successfully extracted and saved article blackassign0057\n",
      "Successfully extracted and saved article blackassign0058\n",
      "Successfully extracted and saved article blackassign0059\n",
      "Successfully extracted and saved article blackassign0060\n",
      "Successfully extracted and saved article blackassign0061\n",
      "Successfully extracted and saved article blackassign0062\n",
      "Successfully extracted and saved article blackassign0063\n",
      "Successfully extracted and saved article blackassign0064\n",
      "Successfully extracted and saved article blackassign0065\n",
      "Successfully extracted and saved article blackassign0066\n",
      "Successfully extracted and saved article blackassign0067\n",
      "Successfully extracted and saved article blackassign0068\n",
      "Successfully extracted and saved article blackassign0069\n",
      "Successfully extracted and saved article blackassign0070\n",
      "Successfully extracted and saved article blackassign0071\n",
      "Successfully extracted and saved article blackassign0072\n",
      "Successfully extracted and saved article blackassign0073\n",
      "Successfully extracted and saved article blackassign0074\n",
      "Successfully extracted and saved article blackassign0075\n",
      "Successfully extracted and saved article blackassign0076\n",
      "Successfully extracted and saved article blackassign0077\n",
      "Successfully extracted and saved article blackassign0078\n",
      "Successfully extracted and saved article blackassign0079\n",
      "Successfully extracted and saved article blackassign0080\n",
      "Successfully extracted and saved article blackassign0081\n",
      "Successfully extracted and saved article blackassign0082\n",
      "Successfully extracted and saved article blackassign0083\n",
      "Successfully extracted and saved article blackassign0084\n",
      "Successfully extracted and saved article blackassign0085\n",
      "Successfully extracted and saved article blackassign0086\n",
      "Successfully extracted and saved article blackassign0087\n",
      "Successfully extracted and saved article blackassign0088\n",
      "Successfully extracted and saved article blackassign0089\n",
      "Successfully extracted and saved article blackassign0090\n",
      "Successfully extracted and saved article blackassign0091\n",
      "Successfully extracted and saved article blackassign0092\n",
      "Successfully extracted and saved article blackassign0093\n",
      "Successfully extracted and saved article blackassign0094\n",
      "Successfully extracted and saved article blackassign0095\n",
      "Successfully extracted and saved article blackassign0096\n",
      "Successfully extracted and saved article blackassign0097\n",
      "Successfully extracted and saved article blackassign0098\n",
      "Successfully extracted and saved article blackassign0099\n",
      "Successfully extracted and saved article blackassign0100\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def extract_article(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Assuming the article title is in <h1> tag and the text is within <p> tags\n",
    "    title = soup.find('h1').get_text(strip=True)\n",
    "    paragraphs = soup.find_all('p')\n",
    "    article_text = ' '.join([para.get_text(strip=True) for para in paragraphs])\n",
    "\n",
    "    return title, article_text\n",
    "\n",
    "def save_article_text(url_id, title, text):\n",
    "    with open(f'{url_id}.txt', 'w', encoding='utf-8') as file:\n",
    "        file.write(f'{title}\\n\\n{text}')\n",
    "\n",
    "# Load URLs from the Excel file\n",
    "input_file = r'C:\\Users\\Sparx\\Downloads\\DataEngg\\Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    try:\n",
    "        title, text = extract_article(url)\n",
    "        save_article_text(url_id, title, text)\n",
    "        print(f'Successfully extracted and saved article {url_id}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to extract article {url_id} from {url}: {e}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56df5bbf-b2fc-488e-bf0b-5ee5f4ff54de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Sparx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sparx\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis completed for article blackassign0001\n",
      "Analysis completed for article blackassign0002\n",
      "Analysis completed for article blackassign0003\n",
      "Analysis completed for article blackassign0004\n",
      "Analysis completed for article blackassign0005\n",
      "Analysis completed for article blackassign0006\n",
      "Analysis completed for article blackassign0007\n",
      "Analysis completed for article blackassign0008\n",
      "Analysis completed for article blackassign0009\n",
      "Analysis completed for article blackassign0010\n",
      "Analysis completed for article blackassign0011\n",
      "Analysis completed for article blackassign0012\n",
      "Analysis completed for article blackassign0013\n",
      "Analysis completed for article blackassign0014\n",
      "Analysis completed for article blackassign0015\n",
      "Analysis completed for article blackassign0016\n",
      "Analysis completed for article blackassign0017\n",
      "Analysis completed for article blackassign0018\n",
      "Analysis completed for article blackassign0019\n",
      "Analysis completed for article blackassign0020\n",
      "Analysis completed for article blackassign0021\n",
      "Analysis completed for article blackassign0022\n",
      "Analysis completed for article blackassign0023\n",
      "Analysis completed for article blackassign0024\n",
      "Analysis completed for article blackassign0025\n",
      "Analysis completed for article blackassign0026\n",
      "Analysis completed for article blackassign0027\n",
      "Analysis completed for article blackassign0028\n",
      "Analysis completed for article blackassign0029\n",
      "Analysis completed for article blackassign0030\n",
      "Analysis completed for article blackassign0031\n",
      "Analysis completed for article blackassign0032\n",
      "Analysis completed for article blackassign0033\n",
      "Analysis completed for article blackassign0034\n",
      "Analysis completed for article blackassign0035\n",
      "Failed to analyze article blackassign0036: [Errno 2] No such file or directory: 'blackassign0036.txt'\n",
      "Analysis completed for article blackassign0037\n",
      "Analysis completed for article blackassign0038\n",
      "Analysis completed for article blackassign0039\n",
      "Analysis completed for article blackassign0040\n",
      "Analysis completed for article blackassign0041\n",
      "Analysis completed for article blackassign0042\n",
      "Analysis completed for article blackassign0043\n",
      "Analysis completed for article blackassign0044\n",
      "Analysis completed for article blackassign0045\n",
      "Analysis completed for article blackassign0046\n",
      "Analysis completed for article blackassign0047\n",
      "Analysis completed for article blackassign0048\n",
      "Failed to analyze article blackassign0049: [Errno 2] No such file or directory: 'blackassign0049.txt'\n",
      "Analysis completed for article blackassign0050\n",
      "Analysis completed for article blackassign0051\n",
      "Analysis completed for article blackassign0052\n",
      "Analysis completed for article blackassign0053\n",
      "Analysis completed for article blackassign0054\n",
      "Analysis completed for article blackassign0055\n",
      "Analysis completed for article blackassign0056\n",
      "Analysis completed for article blackassign0057\n",
      "Analysis completed for article blackassign0058\n",
      "Analysis completed for article blackassign0059\n",
      "Analysis completed for article blackassign0060\n",
      "Analysis completed for article blackassign0061\n",
      "Analysis completed for article blackassign0062\n",
      "Analysis completed for article blackassign0063\n",
      "Analysis completed for article blackassign0064\n",
      "Analysis completed for article blackassign0065\n",
      "Analysis completed for article blackassign0066\n",
      "Analysis completed for article blackassign0067\n",
      "Analysis completed for article blackassign0068\n",
      "Analysis completed for article blackassign0069\n",
      "Analysis completed for article blackassign0070\n",
      "Analysis completed for article blackassign0071\n",
      "Analysis completed for article blackassign0072\n",
      "Analysis completed for article blackassign0073\n",
      "Analysis completed for article blackassign0074\n",
      "Analysis completed for article blackassign0075\n",
      "Analysis completed for article blackassign0076\n",
      "Analysis completed for article blackassign0077\n",
      "Analysis completed for article blackassign0078\n",
      "Analysis completed for article blackassign0079\n",
      "Analysis completed for article blackassign0080\n",
      "Analysis completed for article blackassign0081\n",
      "Analysis completed for article blackassign0082\n",
      "Analysis completed for article blackassign0083\n",
      "Analysis completed for article blackassign0084\n",
      "Analysis completed for article blackassign0085\n",
      "Analysis completed for article blackassign0086\n",
      "Analysis completed for article blackassign0087\n",
      "Analysis completed for article blackassign0088\n",
      "Analysis completed for article blackassign0089\n",
      "Analysis completed for article blackassign0090\n",
      "Analysis completed for article blackassign0091\n",
      "Analysis completed for article blackassign0092\n",
      "Analysis completed for article blackassign0093\n",
      "Analysis completed for article blackassign0094\n",
      "Analysis completed for article blackassign0095\n",
      "Analysis completed for article blackassign0096\n",
      "Analysis completed for article blackassign0097\n",
      "Analysis completed for article blackassign0098\n",
      "Analysis completed for article blackassign0099\n",
      "Analysis completed for article blackassign0100\n",
      "Analysis results saved to Output_Data_Structure.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def load_words(filename):\n",
    "    with open(filename, 'r', encoding='latin-1') as file:\n",
    "        return set(file.read().split())\n",
    "\n",
    "# Load positive and negative words from local text files\n",
    "negative_words = load_words('C:/Users/Sparx/Downloads/DataEngg/MasterDictionary-20240701T111816Z-001/MasterDictionary/negative-words.txt')\n",
    "positive_words = load_words('C:/Users/Sparx/Downloads/DataEngg/MasterDictionary-20240701T111816Z-001/MasterDictionary/positive-words.txt')\n",
    "\n",
    "def text_analysis(text):\n",
    "    # Compute word tokens\n",
    "    words = word_tokenize(text.lower())\n",
    "    num_words = len(words)\n",
    "\n",
    "    # Compute positive and negative scores\n",
    "    positive_score = sum(1 for word in words if word in positive_words)\n",
    "    negative_score = sum(1 for word in words if word in negative_words)\n",
    "\n",
    "    # Compute polarity and subjectivity scores\n",
    "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
    "    subjectivity_score = (positive_score + negative_score) / (num_words + 0.000001)\n",
    "\n",
    "    # Sentence analysis\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "\n",
    "    # Compute complex words and Fog Index\n",
    "    complex_words = [word for word in words if len(word) > 2 and sum(1 for char in word if char in 'aeiou') > 2]\n",
    "    num_complex_words = len(complex_words)\n",
    "    percentage_complex_words = num_complex_words / num_words\n",
    "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
    "\n",
    "    # Additional metrics\n",
    "    avg_words_per_sentence = num_words / num_sentences\n",
    "    syllable_per_word = sum([len(re.findall(r'[aeiouy]+', word)) for word in words]) / num_words\n",
    "    personal_pronouns = len(re.findall(r'\\b(I|we|my|ours|us)\\b', text, re.IGNORECASE))\n",
    "    avg_word_length = sum(len(word) for word in words) / num_words\n",
    "\n",
    "    return {\n",
    "        \"positive_score\": positive_score,\n",
    "        \"negative_score\": negative_score,\n",
    "        \"polarity_score\": polarity_score,\n",
    "        \"subjectivity_score\": subjectivity_score,\n",
    "        \"avg_sentence_length\": avg_sentence_length,\n",
    "        \"percentage_complex_words\": percentage_complex_words,\n",
    "        \"fog_index\": fog_index,\n",
    "        \"avg_words_per_sentence\": avg_words_per_sentence,\n",
    "        \"complex_word_count\": num_complex_words,\n",
    "        \"word_count\": num_words,\n",
    "        \"syllable_per_word\": syllable_per_word,\n",
    "        \"personal_pronouns\": personal_pronouns,\n",
    "        \"avg_word_length\": avg_word_length,\n",
    "    }\n",
    "\n",
    "def save_analysis_results(url_id, analysis):\n",
    "    for key, value in analysis.items():\n",
    "        with open(f'{url_id}_{key}.txt', 'w', encoding='utf-8') as file:\n",
    "            file.write(str(value))\n",
    "\n",
    "# Load input and prepare output DataFrame\n",
    "input_file = r'C:\\Users\\Sparx\\Downloads\\DataEngg\\Input.xlsx'\n",
    "df = pd.read_excel(input_file)\n",
    "results = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    try:\n",
    "        with open(f'{url_id}.txt', 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        analysis = text_analysis(text)\n",
    "        save_analysis_results(url_id, analysis)\n",
    "        results.append({**row, **analysis})\n",
    "        print(f'Analysis completed for article {url_id}')\n",
    "    except Exception as e:\n",
    "        print(f'Failed to analyze article {url_id}: {e}')\n",
    "\n",
    "output_df = pd.DataFrame(results)\n",
    "output_df.to_excel(r'C:\\Users\\Sparx\\Downloads\\DataEngg\\Output_Data_Structure.xlsx', index=False)\n",
    "print('Analysis results saved to Output_Data_Structure.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042751e1-f883-453d-83a8-c6c326aea363",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
